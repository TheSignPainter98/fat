local *

import spec from require 'spec'

type_checkers = {}

typed = (type_spec, value) ->
  if not type_spec?
    error 'cannot typecheck: no type spec provided'

  -- checker_steps = type_checkers[type_spec] ?? do
  --   program = generate_checker_program type_spec
  --   type_checkers[type_spec] = program
  --   program
  checker = type_checker type_spec

  -- print (require 'spec').repr checker

type_checkers = {}
type_checker = (type_spec) ->
  with {}
    -- stack = {}
    lexer = Lexer type_spec
    -- print type lexer!
    -- token_type, token = coroutine.resume lexer
    token_type, token = lexer\next!
    while token?
      print token_type, token
      token_type, token = lexer\next!
    -- for token_type, token in
    --   print token_type, token
    -- idx = 1
    -- if named_type = (type_spec\sub idx)\match '^([0-9]+)'
    --   steps = type_checkers[named_type]
    --   if not steps?
    --     error "cannot typecheck: unknown type '#{named_type}'"
    --   for step in *steps
    --     [] = step
    -- else if (type_spec\sub i)\match '^('
    --   error 'tuples and functions not yet supported'
    -- else if (type_spec )

PAREN_OPEN = <tostring>: => "paren_open"
PAREN_CLOSE = <tostring>: => "paren_close"
BRACE_OPEN = <tostring>: => "brace_open"
BRACE_CLOSE = <tostring>: => "brace_close"
BRACKET_OPEN = <tostring>: => "bracket_close"
BRACKET_CLOSE = <tostring>: => "bracket_close"
COMMA = <tostring>: => "comma"
ARROW = <tostring>: => "arrow"
NAME = <tostring>: => "name"
class Lexer
  new: (type_spec) =>
    @done = false
    @peeked = nil
    @tokens = coroutine.wrap ->
      original_type_spec = type_spec
      while #type_spec > 0
        token_type, value = if whitespace = type_spec\match '^[ \t\r\n]'
          nil, whitespace
        else if type_spec\match '^%('
          PAREN_OPEN, '('
        else if type_spec\match '^%)'
          PAREN_CLOSE, ')'
        else if type_spec\match '^,'
          COMMA, ','
        else if type_spec\match '^{'
          BRACE_OPEN, '{'
        else if type_spec\match '^}'
          BRACE_CLOSE, '}'
        else if type_spec\match '^%['
          BRACKET_OPEN, '['
        else if type_spec\match '^]'
          BRACKET_CLOSE, ']'
        else if type_spec\match '^->'
          ARROW, '->'
        else if name = type_spec\match '^([a-zA-Z_][a-zA-Z0-9_]*)'
          NAME, name
        else
          error "unrecognised character '#{type_spec\sub 1, 1}' in type spec '#{original_type_spec}"

        type_spec = type_spec\sub #value + 1
        switch token_type
          when nil
            continue -- continue
          when NAME
            coroutine.yield { :token_type, :value }
          else
            coroutine.yield { :token_type }

  peek: =>
    if @done
      return nil

    if @peeked?
      return @peeked

    @peeked = @tokens!
    if not @peeked
      @done = true
    @peeked

  next: =>
    if @done
      return nil

    if @peeked?
      peeked = @peeked
      @peeked = nil
      peeked
    else
      @tokens!

declare_type = (name, type_spec) ->
  error 'todo'

spec ->
  import assert_that, expect_that, describe, it, matchers from require 'spec'
  import anything, deep_eq, eq, errors, match, matches, no_errors from matchers

  describe 'lex', ->
    tokens = (raw) ->
      with {}
        for token in (Lexer raw).tokens
          [] = token

    it 'emits simple types', ->
      simple_types =
        * type nil
        * type false
        * type 0
        * type ""
      for simple_type in *simple_types
        expect_that (tokens simple_type), deep_eq {
          { token_type: NAME, value: simple_type },
        }

    it 'emits strucural tokens', ->
      expect_that (tokens '(),{}[]->'), deep_eq {
        { token_type: PAREN_OPEN },
        { token_type: PAREN_CLOSE },
        { token_type: COMMA },
        { token_type: BRACE_OPEN },
        { token_type: BRACE_CLOSE },
        { token_type: BRACKET_OPEN },
        { token_type: BRACKET_CLOSE },
        { token_type: ARROW },
      }

    it 'ignores whitespace', ->
      expect_that (tokens ' (\tstring\r)\n-> string '), deep_eq {
        { token_type: PAREN_OPEN },
        { token_type: NAME, value: "string" },
        { token_type: PAREN_CLOSE },
        { token_type: ARROW },
        { token_type: NAME, value: "string" },
      }

    it 'rejects unrecognised characters', ->
      expect_that (-> tokens '"'), errors matches [[unrecognised character '"']]
      expect_that (-> tokens '1'), errors matches [[unrecognised character '1']]

    describe ':peek', ->
      it 'matches :next', ->
        lexer = Lexer '()'
        assert_that lexer\peek!, deep_eq { token_type: PAREN_OPEN }
        assert_that lexer\peek!, deep_eq { token_type: PAREN_OPEN }
        assert_that lexer\next!, deep_eq { token_type: PAREN_OPEN }
        assert_that lexer\peek!, deep_eq { token_type: PAREN_CLOSE }
        assert_that lexer\peek!, deep_eq { token_type: PAREN_CLOSE }
        assert_that lexer\next!, deep_eq { token_type: PAREN_CLOSE }

      it 'returns nil on empty peek', ->
        lexer = Lexer ''
        expect_that lexer\peek!, eq nil
        expect_that lexer\peek!, eq nil

  -- describe 'typed', ->
  --   it 'requires two arguments', ->
  --     expect_that (-> typed!), errors matches 'cannot typecheck: no type spec provided'
  --   --
  --   -- it 'handles nils', ->
  --   --   expect_that (-> typed 'nil', nil), no_errors!
  --   --   expect_that (-> typed 'nil', 123), errors anything!
  --
  --   it 'handles numbers', ->
  --     expect_that (-> typed 'number', nil), errors anything!
  --     expect_that (-> typed 'number', 123), no_errors!

(require 'spec').run_tests!

-- type_checker '(string, string) -> boolean'
